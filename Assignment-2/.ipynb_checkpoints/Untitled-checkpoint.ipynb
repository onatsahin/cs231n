{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  means: ', x.mean(axis=axis))\n",
    "    print('  stds:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "y_val:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 49000) loss: 14.241538\n",
      "(Epoch 0 / 100) train acc: 0.085000; val_acc: 0.099000\n",
      "(Iteration 101 / 49000) loss: 11.541552\n",
      "(Iteration 201 / 49000) loss: 9.330399\n",
      "(Iteration 301 / 49000) loss: 7.542473\n",
      "(Iteration 401 / 49000) loss: 6.295383\n",
      "(Epoch 1 / 100) train acc: 0.351000; val_acc: 0.343000\n",
      "(Iteration 501 / 49000) loss: 5.280406\n",
      "(Iteration 601 / 49000) loss: 4.665027\n",
      "(Iteration 701 / 49000) loss: 3.968175\n",
      "(Iteration 801 / 49000) loss: 3.570175\n",
      "(Iteration 901 / 49000) loss: 3.160898\n",
      "(Epoch 2 / 100) train acc: 0.410000; val_acc: 0.394000\n",
      "(Iteration 1001 / 49000) loss: 3.071476\n",
      "(Iteration 1101 / 49000) loss: 2.821758\n",
      "(Iteration 1201 / 49000) loss: 2.587249\n",
      "(Iteration 1301 / 49000) loss: 2.525726\n",
      "(Iteration 1401 / 49000) loss: 2.467916\n",
      "(Epoch 3 / 100) train acc: 0.419000; val_acc: 0.431000\n",
      "(Iteration 1501 / 49000) loss: 2.475306\n",
      "(Iteration 1601 / 49000) loss: 2.311577\n",
      "(Iteration 1701 / 49000) loss: 2.366257\n",
      "(Iteration 1801 / 49000) loss: 2.189463\n",
      "(Iteration 1901 / 49000) loss: 2.215221\n",
      "(Epoch 4 / 100) train acc: 0.459000; val_acc: 0.440000\n",
      "(Iteration 2001 / 49000) loss: 2.088775\n",
      "(Iteration 2101 / 49000) loss: 2.085917\n",
      "(Iteration 2201 / 49000) loss: 2.152964\n",
      "(Iteration 2301 / 49000) loss: 1.952979\n",
      "(Iteration 2401 / 49000) loss: 2.108146\n",
      "(Epoch 5 / 100) train acc: 0.467000; val_acc: 0.460000\n",
      "(Iteration 2501 / 49000) loss: 2.061642\n",
      "(Iteration 2601 / 49000) loss: 1.847624\n",
      "(Iteration 2701 / 49000) loss: 2.058053\n",
      "(Iteration 2801 / 49000) loss: 2.013328\n",
      "(Iteration 2901 / 49000) loss: 2.003282\n",
      "(Epoch 6 / 100) train acc: 0.475000; val_acc: 0.485000\n",
      "(Iteration 3001 / 49000) loss: 1.901694\n",
      "(Iteration 3101 / 49000) loss: 1.819028\n",
      "(Iteration 3201 / 49000) loss: 1.922481\n",
      "(Iteration 3301 / 49000) loss: 1.886620\n",
      "(Iteration 3401 / 49000) loss: 1.916240\n",
      "(Epoch 7 / 100) train acc: 0.497000; val_acc: 0.484000\n",
      "(Iteration 3501 / 49000) loss: 1.909797\n",
      "(Iteration 3601 / 49000) loss: 1.974936\n",
      "(Iteration 3701 / 49000) loss: 1.815065\n",
      "(Iteration 3801 / 49000) loss: 1.848235\n",
      "(Iteration 3901 / 49000) loss: 1.696029\n",
      "(Epoch 8 / 100) train acc: 0.467000; val_acc: 0.487000\n",
      "(Iteration 4001 / 49000) loss: 1.715025\n",
      "(Iteration 4101 / 49000) loss: 1.703394\n",
      "(Iteration 4201 / 49000) loss: 1.637590\n",
      "(Iteration 4301 / 49000) loss: 1.830325\n",
      "(Iteration 4401 / 49000) loss: 1.750201\n",
      "(Epoch 9 / 100) train acc: 0.494000; val_acc: 0.485000\n",
      "(Iteration 4501 / 49000) loss: 1.869625\n",
      "(Iteration 4601 / 49000) loss: 1.774868\n",
      "(Iteration 4701 / 49000) loss: 1.811054\n",
      "(Iteration 4801 / 49000) loss: 1.894076\n",
      "(Epoch 10 / 100) train acc: 0.507000; val_acc: 0.490000\n",
      "(Iteration 4901 / 49000) loss: 1.922766\n",
      "(Iteration 5001 / 49000) loss: 1.752206\n",
      "(Iteration 5101 / 49000) loss: 1.863806\n",
      "(Iteration 5201 / 49000) loss: 1.774513\n",
      "(Iteration 5301 / 49000) loss: 1.724917\n",
      "(Epoch 11 / 100) train acc: 0.511000; val_acc: 0.509000\n",
      "(Iteration 5401 / 49000) loss: 1.790038\n",
      "(Iteration 5501 / 49000) loss: 1.668280\n",
      "(Iteration 5601 / 49000) loss: 1.655764\n",
      "(Iteration 5701 / 49000) loss: 1.727648\n",
      "(Iteration 5801 / 49000) loss: 1.654932\n",
      "(Epoch 12 / 100) train acc: 0.507000; val_acc: 0.517000\n",
      "(Iteration 5901 / 49000) loss: 1.608206\n",
      "(Iteration 6001 / 49000) loss: 1.668182\n",
      "(Iteration 6101 / 49000) loss: 1.849262\n",
      "(Iteration 6201 / 49000) loss: 1.694719\n",
      "(Iteration 6301 / 49000) loss: 1.717883\n",
      "(Epoch 13 / 100) train acc: 0.510000; val_acc: 0.502000\n",
      "(Iteration 6401 / 49000) loss: 1.688093\n",
      "(Iteration 6501 / 49000) loss: 1.765422\n",
      "(Iteration 6601 / 49000) loss: 1.599758\n",
      "(Iteration 6701 / 49000) loss: 1.724599\n",
      "(Iteration 6801 / 49000) loss: 1.716467\n",
      "(Epoch 14 / 100) train acc: 0.521000; val_acc: 0.507000\n",
      "(Iteration 6901 / 49000) loss: 1.602186\n",
      "(Iteration 7001 / 49000) loss: 1.717440\n",
      "(Iteration 7101 / 49000) loss: 1.806335\n",
      "(Iteration 7201 / 49000) loss: 1.742405\n",
      "(Iteration 7301 / 49000) loss: 1.786266\n",
      "(Epoch 15 / 100) train acc: 0.538000; val_acc: 0.519000\n",
      "(Iteration 7401 / 49000) loss: 1.667855\n",
      "(Iteration 7501 / 49000) loss: 1.689040\n",
      "(Iteration 7601 / 49000) loss: 1.820710\n",
      "(Iteration 7701 / 49000) loss: 1.671407\n",
      "(Iteration 7801 / 49000) loss: 1.888381\n",
      "(Epoch 16 / 100) train acc: 0.523000; val_acc: 0.507000\n",
      "(Iteration 7901 / 49000) loss: 1.648190\n",
      "(Iteration 8001 / 49000) loss: 1.572401\n",
      "(Iteration 8101 / 49000) loss: 1.680637\n",
      "(Iteration 8201 / 49000) loss: 1.800234\n",
      "(Iteration 8301 / 49000) loss: 1.588733\n",
      "(Epoch 17 / 100) train acc: 0.563000; val_acc: 0.523000\n",
      "(Iteration 8401 / 49000) loss: 1.675769\n",
      "(Iteration 8501 / 49000) loss: 1.742785\n",
      "(Iteration 8601 / 49000) loss: 1.616420\n",
      "(Iteration 8701 / 49000) loss: 1.625469\n",
      "(Iteration 8801 / 49000) loss: 1.693620\n",
      "(Epoch 18 / 100) train acc: 0.540000; val_acc: 0.517000\n",
      "(Iteration 8901 / 49000) loss: 1.770097\n",
      "(Iteration 9001 / 49000) loss: 1.751286\n",
      "(Iteration 9101 / 49000) loss: 1.725393\n",
      "(Iteration 9201 / 49000) loss: 1.554439\n",
      "(Iteration 9301 / 49000) loss: 1.522108\n",
      "(Epoch 19 / 100) train acc: 0.524000; val_acc: 0.517000\n",
      "(Iteration 9401 / 49000) loss: 1.625885\n",
      "(Iteration 9501 / 49000) loss: 1.845396\n",
      "(Iteration 9601 / 49000) loss: 1.619614\n",
      "(Iteration 9701 / 49000) loss: 1.623268\n",
      "(Epoch 20 / 100) train acc: 0.563000; val_acc: 0.517000\n",
      "(Iteration 9801 / 49000) loss: 1.616931\n",
      "(Iteration 9901 / 49000) loss: 1.607526\n",
      "(Iteration 10001 / 49000) loss: 1.666429\n",
      "(Iteration 10101 / 49000) loss: 1.623070\n",
      "(Iteration 10201 / 49000) loss: 1.587064\n",
      "(Epoch 21 / 100) train acc: 0.559000; val_acc: 0.525000\n",
      "(Iteration 10301 / 49000) loss: 1.891504\n",
      "(Iteration 10401 / 49000) loss: 1.703000\n",
      "(Iteration 10501 / 49000) loss: 1.508857\n",
      "(Iteration 10601 / 49000) loss: 1.416593\n",
      "(Iteration 10701 / 49000) loss: 1.540587\n",
      "(Epoch 22 / 100) train acc: 0.539000; val_acc: 0.524000\n",
      "(Iteration 10801 / 49000) loss: 1.761911\n",
      "(Iteration 10901 / 49000) loss: 1.628379\n",
      "(Iteration 11001 / 49000) loss: 1.627991\n",
      "(Iteration 11101 / 49000) loss: 1.564691\n",
      "(Iteration 11201 / 49000) loss: 1.625905\n",
      "(Epoch 23 / 100) train acc: 0.558000; val_acc: 0.543000\n",
      "(Iteration 11301 / 49000) loss: 1.618759\n",
      "(Iteration 11401 / 49000) loss: 1.639840\n",
      "(Iteration 11501 / 49000) loss: 1.784446\n",
      "(Iteration 11601 / 49000) loss: 1.660533\n",
      "(Iteration 11701 / 49000) loss: 1.408713\n",
      "(Epoch 24 / 100) train acc: 0.573000; val_acc: 0.541000\n",
      "(Iteration 11801 / 49000) loss: 1.686419\n",
      "(Iteration 11901 / 49000) loss: 1.742970\n",
      "(Iteration 12001 / 49000) loss: 1.649732\n",
      "(Iteration 12101 / 49000) loss: 1.724077\n",
      "(Iteration 12201 / 49000) loss: 1.610003\n",
      "(Epoch 25 / 100) train acc: 0.558000; val_acc: 0.537000\n",
      "(Iteration 12301 / 49000) loss: 1.659544\n",
      "(Iteration 12401 / 49000) loss: 1.544509\n",
      "(Iteration 12501 / 49000) loss: 1.613499\n",
      "(Iteration 12601 / 49000) loss: 1.733999\n",
      "(Iteration 12701 / 49000) loss: 1.434673\n",
      "(Epoch 26 / 100) train acc: 0.602000; val_acc: 0.547000\n",
      "(Iteration 12801 / 49000) loss: 1.637767\n",
      "(Iteration 12901 / 49000) loss: 1.655850\n",
      "(Iteration 13001 / 49000) loss: 1.518690\n",
      "(Iteration 13101 / 49000) loss: 1.682807\n",
      "(Iteration 13201 / 49000) loss: 1.637633\n",
      "(Epoch 27 / 100) train acc: 0.600000; val_acc: 0.551000\n",
      "(Iteration 13301 / 49000) loss: 1.511723\n",
      "(Iteration 13401 / 49000) loss: 1.470422\n",
      "(Iteration 13501 / 49000) loss: 1.561425\n",
      "(Iteration 13601 / 49000) loss: 1.541396\n",
      "(Iteration 13701 / 49000) loss: 1.531398\n",
      "(Epoch 28 / 100) train acc: 0.595000; val_acc: 0.535000\n",
      "(Iteration 13801 / 49000) loss: 1.582459\n",
      "(Iteration 13901 / 49000) loss: 1.574870\n",
      "(Iteration 14001 / 49000) loss: 1.695156\n",
      "(Iteration 14101 / 49000) loss: 1.714345\n",
      "(Iteration 14201 / 49000) loss: 1.544893\n",
      "(Epoch 29 / 100) train acc: 0.582000; val_acc: 0.543000\n",
      "(Iteration 14301 / 49000) loss: 1.634873\n",
      "(Iteration 14401 / 49000) loss: 1.645840\n",
      "(Iteration 14501 / 49000) loss: 1.575168\n",
      "(Iteration 14601 / 49000) loss: 1.381392\n",
      "(Epoch 30 / 100) train acc: 0.590000; val_acc: 0.541000\n",
      "(Iteration 14701 / 49000) loss: 1.542457\n",
      "(Iteration 14801 / 49000) loss: 1.447530\n",
      "(Iteration 14901 / 49000) loss: 1.347247\n",
      "(Iteration 15001 / 49000) loss: 1.465749\n",
      "(Iteration 15101 / 49000) loss: 1.662703\n",
      "(Epoch 31 / 100) train acc: 0.631000; val_acc: 0.556000\n",
      "(Iteration 15201 / 49000) loss: 1.363970\n",
      "(Iteration 15301 / 49000) loss: 1.602206\n",
      "(Iteration 15401 / 49000) loss: 1.277889\n",
      "(Iteration 15501 / 49000) loss: 1.537789\n",
      "(Iteration 15601 / 49000) loss: 1.793511\n",
      "(Epoch 32 / 100) train acc: 0.599000; val_acc: 0.551000\n",
      "(Iteration 15701 / 49000) loss: 1.851465\n",
      "(Iteration 15801 / 49000) loss: 1.650050\n",
      "(Iteration 15901 / 49000) loss: 1.456943\n",
      "(Iteration 16001 / 49000) loss: 1.382935\n",
      "(Iteration 16101 / 49000) loss: 1.403963\n",
      "(Epoch 33 / 100) train acc: 0.616000; val_acc: 0.549000\n",
      "(Iteration 16201 / 49000) loss: 1.571530\n",
      "(Iteration 16301 / 49000) loss: 1.532502\n",
      "(Iteration 16401 / 49000) loss: 1.519970\n",
      "(Iteration 16501 / 49000) loss: 1.561628\n",
      "(Iteration 16601 / 49000) loss: 1.758622\n",
      "(Epoch 34 / 100) train acc: 0.590000; val_acc: 0.552000\n",
      "(Iteration 16701 / 49000) loss: 1.677641\n",
      "(Iteration 16801 / 49000) loss: 1.562995\n",
      "(Iteration 16901 / 49000) loss: 1.470288\n",
      "(Iteration 17001 / 49000) loss: 1.490062\n",
      "(Iteration 17101 / 49000) loss: 1.435601\n",
      "(Epoch 35 / 100) train acc: 0.616000; val_acc: 0.539000\n",
      "(Iteration 17201 / 49000) loss: 1.534773\n",
      "(Iteration 17301 / 49000) loss: 1.458352\n",
      "(Iteration 17401 / 49000) loss: 1.570710\n",
      "(Iteration 17501 / 49000) loss: 1.508969\n",
      "(Iteration 17601 / 49000) loss: 1.482057\n",
      "(Epoch 36 / 100) train acc: 0.635000; val_acc: 0.545000\n",
      "(Iteration 17701 / 49000) loss: 1.571405\n",
      "(Iteration 17801 / 49000) loss: 1.390876\n",
      "(Iteration 17901 / 49000) loss: 1.420965\n",
      "(Iteration 18001 / 49000) loss: 1.377836\n",
      "(Iteration 18101 / 49000) loss: 1.318662\n",
      "(Epoch 37 / 100) train acc: 0.612000; val_acc: 0.554000\n",
      "(Iteration 18201 / 49000) loss: 1.433979\n",
      "(Iteration 18301 / 49000) loss: 1.332708\n",
      "(Iteration 18401 / 49000) loss: 1.445460\n",
      "(Iteration 18501 / 49000) loss: 1.471917\n",
      "(Iteration 18601 / 49000) loss: 1.591937\n",
      "(Epoch 38 / 100) train acc: 0.626000; val_acc: 0.553000\n",
      "(Iteration 18701 / 49000) loss: 1.477295\n",
      "(Iteration 18801 / 49000) loss: 1.598772\n",
      "(Iteration 18901 / 49000) loss: 1.546571\n",
      "(Iteration 19001 / 49000) loss: 1.533243\n",
      "(Iteration 19101 / 49000) loss: 1.383169\n",
      "(Epoch 39 / 100) train acc: 0.598000; val_acc: 0.548000\n",
      "(Iteration 19201 / 49000) loss: 1.416750\n",
      "(Iteration 19301 / 49000) loss: 1.488705\n",
      "(Iteration 19401 / 49000) loss: 1.430762\n",
      "(Iteration 19501 / 49000) loss: 1.485033\n",
      "(Epoch 40 / 100) train acc: 0.631000; val_acc: 0.550000\n",
      "(Iteration 19601 / 49000) loss: 1.378731\n",
      "(Iteration 19701 / 49000) loss: 1.491239\n",
      "(Iteration 19801 / 49000) loss: 1.447331\n",
      "(Iteration 19901 / 49000) loss: 1.382423\n",
      "(Iteration 20001 / 49000) loss: 1.473184\n",
      "(Epoch 41 / 100) train acc: 0.609000; val_acc: 0.549000\n",
      "(Iteration 20101 / 49000) loss: 1.537049\n",
      "(Iteration 20201 / 49000) loss: 1.358301\n",
      "(Iteration 20301 / 49000) loss: 1.499452\n",
      "(Iteration 20401 / 49000) loss: 1.492886\n",
      "(Iteration 20501 / 49000) loss: 1.475585\n",
      "(Epoch 42 / 100) train acc: 0.621000; val_acc: 0.565000\n",
      "(Iteration 20601 / 49000) loss: 1.573143\n",
      "(Iteration 20701 / 49000) loss: 1.325089\n",
      "(Iteration 20801 / 49000) loss: 1.503829\n",
      "(Iteration 20901 / 49000) loss: 1.590562\n",
      "(Iteration 21001 / 49000) loss: 1.612019\n",
      "(Epoch 43 / 100) train acc: 0.645000; val_acc: 0.559000\n",
      "(Iteration 21101 / 49000) loss: 1.483295\n",
      "(Iteration 21201 / 49000) loss: 1.447120\n",
      "(Iteration 21301 / 49000) loss: 1.335325\n",
      "(Iteration 21401 / 49000) loss: 1.424722\n",
      "(Iteration 21501 / 49000) loss: 1.342497\n",
      "(Epoch 44 / 100) train acc: 0.618000; val_acc: 0.557000\n",
      "(Iteration 21601 / 49000) loss: 1.443634\n",
      "(Iteration 21701 / 49000) loss: 1.543104\n",
      "(Iteration 21801 / 49000) loss: 1.473365\n",
      "(Iteration 21901 / 49000) loss: 1.428245\n",
      "(Iteration 22001 / 49000) loss: 1.425531\n",
      "(Epoch 45 / 100) train acc: 0.621000; val_acc: 0.554000\n",
      "(Iteration 22101 / 49000) loss: 1.390579\n",
      "(Iteration 22201 / 49000) loss: 1.272820\n",
      "(Iteration 22301 / 49000) loss: 1.419407\n",
      "(Iteration 22401 / 49000) loss: 1.351458\n",
      "(Iteration 22501 / 49000) loss: 1.379665\n",
      "(Epoch 46 / 100) train acc: 0.647000; val_acc: 0.548000\n",
      "(Iteration 22601 / 49000) loss: 1.273051\n",
      "(Iteration 22701 / 49000) loss: 1.438566\n",
      "(Iteration 22801 / 49000) loss: 1.627692\n",
      "(Iteration 22901 / 49000) loss: 1.159374\n",
      "(Iteration 23001 / 49000) loss: 1.580947\n",
      "(Epoch 47 / 100) train acc: 0.644000; val_acc: 0.562000\n",
      "(Iteration 23101 / 49000) loss: 1.460448\n",
      "(Iteration 23201 / 49000) loss: 1.408704\n",
      "(Iteration 23301 / 49000) loss: 1.504722\n",
      "(Iteration 23401 / 49000) loss: 1.319027\n",
      "(Iteration 23501 / 49000) loss: 1.513625\n",
      "(Epoch 48 / 100) train acc: 0.632000; val_acc: 0.563000\n",
      "(Iteration 23601 / 49000) loss: 1.515121\n",
      "(Iteration 23701 / 49000) loss: 1.361562\n",
      "(Iteration 23801 / 49000) loss: 1.699025\n",
      "(Iteration 23901 / 49000) loss: 1.289949\n",
      "(Iteration 24001 / 49000) loss: 1.426295\n",
      "(Epoch 49 / 100) train acc: 0.620000; val_acc: 0.559000\n",
      "(Iteration 24101 / 49000) loss: 1.583815\n",
      "(Iteration 24201 / 49000) loss: 1.484931\n",
      "(Iteration 24301 / 49000) loss: 1.384586\n",
      "(Iteration 24401 / 49000) loss: 1.514326\n",
      "(Epoch 50 / 100) train acc: 0.659000; val_acc: 0.553000\n",
      "(Iteration 24501 / 49000) loss: 1.432930\n",
      "(Iteration 24601 / 49000) loss: 1.357842\n",
      "(Iteration 24701 / 49000) loss: 1.499260\n",
      "(Iteration 24801 / 49000) loss: 1.219388\n",
      "(Iteration 24901 / 49000) loss: 1.521815\n",
      "(Epoch 51 / 100) train acc: 0.644000; val_acc: 0.566000\n",
      "(Iteration 25001 / 49000) loss: 1.248725\n",
      "(Iteration 25101 / 49000) loss: 1.465299\n",
      "(Iteration 25201 / 49000) loss: 1.170636\n",
      "(Iteration 25301 / 49000) loss: 1.331208\n",
      "(Iteration 25401 / 49000) loss: 1.293113\n",
      "(Epoch 52 / 100) train acc: 0.660000; val_acc: 0.563000\n",
      "(Iteration 25501 / 49000) loss: 1.541236\n",
      "(Iteration 25601 / 49000) loss: 1.498426\n",
      "(Iteration 25701 / 49000) loss: 1.403467\n",
      "(Iteration 25801 / 49000) loss: 1.352251\n",
      "(Iteration 25901 / 49000) loss: 1.366331\n",
      "(Epoch 53 / 100) train acc: 0.657000; val_acc: 0.558000\n",
      "(Iteration 26001 / 49000) loss: 1.267155\n",
      "(Iteration 26101 / 49000) loss: 1.269804\n",
      "(Iteration 26201 / 49000) loss: 1.333408\n",
      "(Iteration 26301 / 49000) loss: 1.365941\n",
      "(Iteration 26401 / 49000) loss: 1.199138\n",
      "(Epoch 54 / 100) train acc: 0.652000; val_acc: 0.554000\n",
      "(Iteration 26501 / 49000) loss: 1.400851\n",
      "(Iteration 26601 / 49000) loss: 1.495032\n",
      "(Iteration 26701 / 49000) loss: 1.439023\n",
      "(Iteration 26801 / 49000) loss: 1.335123\n",
      "(Iteration 26901 / 49000) loss: 1.358553\n",
      "(Epoch 55 / 100) train acc: 0.666000; val_acc: 0.571000\n",
      "(Iteration 27001 / 49000) loss: 1.345294\n",
      "(Iteration 27101 / 49000) loss: 1.562106\n",
      "(Iteration 27201 / 49000) loss: 1.452247\n",
      "(Iteration 27301 / 49000) loss: 1.471372\n",
      "(Iteration 27401 / 49000) loss: 1.269921\n",
      "(Epoch 56 / 100) train acc: 0.675000; val_acc: 0.564000\n",
      "(Iteration 27501 / 49000) loss: 1.374603\n",
      "(Iteration 27601 / 49000) loss: 1.583580\n",
      "(Iteration 27701 / 49000) loss: 1.569356\n",
      "(Iteration 27801 / 49000) loss: 1.231834\n",
      "(Iteration 27901 / 49000) loss: 1.243516\n",
      "(Epoch 57 / 100) train acc: 0.665000; val_acc: 0.561000\n",
      "(Iteration 28001 / 49000) loss: 1.415813\n",
      "(Iteration 28101 / 49000) loss: 1.517692\n",
      "(Iteration 28201 / 49000) loss: 1.531354\n",
      "(Iteration 28301 / 49000) loss: 1.307568\n",
      "(Iteration 28401 / 49000) loss: 1.337926\n",
      "(Epoch 58 / 100) train acc: 0.662000; val_acc: 0.565000\n",
      "(Iteration 28501 / 49000) loss: 1.473114\n",
      "(Iteration 28601 / 49000) loss: 1.345231\n",
      "(Iteration 28701 / 49000) loss: 1.431188\n",
      "(Iteration 28801 / 49000) loss: 1.401725\n",
      "(Iteration 28901 / 49000) loss: 1.552672\n",
      "(Epoch 59 / 100) train acc: 0.681000; val_acc: 0.563000\n",
      "(Iteration 29001 / 49000) loss: 1.459795\n",
      "(Iteration 29101 / 49000) loss: 1.251376\n",
      "(Iteration 29201 / 49000) loss: 1.355472\n",
      "(Iteration 29301 / 49000) loss: 1.294140\n",
      "(Epoch 60 / 100) train acc: 0.665000; val_acc: 0.561000\n",
      "(Iteration 29401 / 49000) loss: 1.323999\n",
      "(Iteration 29501 / 49000) loss: 1.526611\n",
      "(Iteration 29601 / 49000) loss: 1.161626\n",
      "(Iteration 29701 / 49000) loss: 1.181284\n",
      "(Iteration 29801 / 49000) loss: 1.469639\n",
      "(Epoch 61 / 100) train acc: 0.650000; val_acc: 0.556000\n",
      "(Iteration 29901 / 49000) loss: 1.336115\n",
      "(Iteration 30001 / 49000) loss: 1.371335\n",
      "(Iteration 30101 / 49000) loss: 1.393920\n",
      "(Iteration 30201 / 49000) loss: 1.557299\n",
      "(Iteration 30301 / 49000) loss: 1.160732\n",
      "(Epoch 62 / 100) train acc: 0.656000; val_acc: 0.568000\n",
      "(Iteration 30401 / 49000) loss: 1.445527\n",
      "(Iteration 30501 / 49000) loss: 1.260076\n",
      "(Iteration 30601 / 49000) loss: 1.229336\n",
      "(Iteration 30701 / 49000) loss: 1.275917\n",
      "(Iteration 30801 / 49000) loss: 1.367409\n",
      "(Epoch 63 / 100) train acc: 0.698000; val_acc: 0.572000\n",
      "(Iteration 30901 / 49000) loss: 1.383364\n",
      "(Iteration 31001 / 49000) loss: 1.257920\n",
      "(Iteration 31101 / 49000) loss: 1.287775\n",
      "(Iteration 31201 / 49000) loss: 1.345585\n",
      "(Iteration 31301 / 49000) loss: 1.363580\n",
      "(Epoch 64 / 100) train acc: 0.705000; val_acc: 0.573000\n",
      "(Iteration 31401 / 49000) loss: 1.445600\n",
      "(Iteration 31501 / 49000) loss: 1.410893\n",
      "(Iteration 31601 / 49000) loss: 1.382123\n",
      "(Iteration 31701 / 49000) loss: 1.389830\n",
      "(Iteration 31801 / 49000) loss: 1.388595\n",
      "(Epoch 65 / 100) train acc: 0.702000; val_acc: 0.562000\n",
      "(Iteration 31901 / 49000) loss: 1.303839\n",
      "(Iteration 32001 / 49000) loss: 1.436830\n",
      "(Iteration 32101 / 49000) loss: 1.260506\n",
      "(Iteration 32201 / 49000) loss: 1.225070\n",
      "(Iteration 32301 / 49000) loss: 1.401678\n",
      "(Epoch 66 / 100) train acc: 0.653000; val_acc: 0.573000\n",
      "(Iteration 32401 / 49000) loss: 1.262555\n",
      "(Iteration 32501 / 49000) loss: 1.343209\n",
      "(Iteration 32601 / 49000) loss: 1.165395\n",
      "(Iteration 32701 / 49000) loss: 1.186655\n",
      "(Iteration 32801 / 49000) loss: 1.309032\n",
      "(Epoch 67 / 100) train acc: 0.681000; val_acc: 0.564000\n",
      "(Iteration 32901 / 49000) loss: 1.347851\n",
      "(Iteration 33001 / 49000) loss: 1.083497\n",
      "(Iteration 33101 / 49000) loss: 1.389991\n",
      "(Iteration 33201 / 49000) loss: 1.448904\n",
      "(Iteration 33301 / 49000) loss: 1.197714\n",
      "(Epoch 68 / 100) train acc: 0.674000; val_acc: 0.579000\n",
      "(Iteration 33401 / 49000) loss: 1.284935\n",
      "(Iteration 33501 / 49000) loss: 1.468160\n",
      "(Iteration 33601 / 49000) loss: 1.145201\n",
      "(Iteration 33701 / 49000) loss: 1.426378\n",
      "(Iteration 33801 / 49000) loss: 1.111482\n",
      "(Epoch 69 / 100) train acc: 0.678000; val_acc: 0.580000\n",
      "(Iteration 33901 / 49000) loss: 1.116352\n",
      "(Iteration 34001 / 49000) loss: 1.171022\n",
      "(Iteration 34101 / 49000) loss: 1.641891\n",
      "(Iteration 34201 / 49000) loss: 1.390654\n",
      "(Epoch 70 / 100) train acc: 0.700000; val_acc: 0.572000\n",
      "(Iteration 34301 / 49000) loss: 1.415927\n",
      "(Iteration 34401 / 49000) loss: 1.057329\n",
      "(Iteration 34501 / 49000) loss: 1.448013\n",
      "(Iteration 34601 / 49000) loss: 1.277657\n",
      "(Iteration 34701 / 49000) loss: 1.201501\n",
      "(Epoch 71 / 100) train acc: 0.691000; val_acc: 0.561000\n",
      "(Iteration 34801 / 49000) loss: 1.425360\n",
      "(Iteration 34901 / 49000) loss: 1.245379\n",
      "(Iteration 35001 / 49000) loss: 1.643996\n",
      "(Iteration 35101 / 49000) loss: 1.271130\n",
      "(Iteration 35201 / 49000) loss: 1.244670\n",
      "(Epoch 72 / 100) train acc: 0.694000; val_acc: 0.567000\n",
      "(Iteration 35301 / 49000) loss: 1.360706\n",
      "(Iteration 35401 / 49000) loss: 1.257065\n",
      "(Iteration 35501 / 49000) loss: 1.089946\n",
      "(Iteration 35601 / 49000) loss: 1.368890\n",
      "(Iteration 35701 / 49000) loss: 1.337031\n",
      "(Epoch 73 / 100) train acc: 0.702000; val_acc: 0.564000\n",
      "(Iteration 35801 / 49000) loss: 1.221744\n",
      "(Iteration 35901 / 49000) loss: 1.309949\n",
      "(Iteration 36001 / 49000) loss: 1.259797\n",
      "(Iteration 36101 / 49000) loss: 1.357876\n",
      "(Iteration 36201 / 49000) loss: 1.288327\n",
      "(Epoch 74 / 100) train acc: 0.709000; val_acc: 0.561000\n",
      "(Iteration 36301 / 49000) loss: 1.443134\n",
      "(Iteration 36401 / 49000) loss: 1.411160\n",
      "(Iteration 36501 / 49000) loss: 1.260861\n",
      "(Iteration 36601 / 49000) loss: 1.263237\n",
      "(Iteration 36701 / 49000) loss: 1.341297\n",
      "(Epoch 75 / 100) train acc: 0.701000; val_acc: 0.569000\n",
      "(Iteration 36801 / 49000) loss: 1.272327\n",
      "(Iteration 36901 / 49000) loss: 1.474049\n",
      "(Iteration 37001 / 49000) loss: 1.589947\n",
      "(Iteration 37101 / 49000) loss: 1.088037\n",
      "(Iteration 37201 / 49000) loss: 1.392748\n",
      "(Epoch 76 / 100) train acc: 0.690000; val_acc: 0.574000\n",
      "(Iteration 37301 / 49000) loss: 1.265574\n",
      "(Iteration 37401 / 49000) loss: 1.369507\n",
      "(Iteration 37501 / 49000) loss: 1.120403\n",
      "(Iteration 37601 / 49000) loss: 1.374381\n",
      "(Iteration 37701 / 49000) loss: 1.328917\n",
      "(Epoch 77 / 100) train acc: 0.681000; val_acc: 0.570000\n",
      "(Iteration 37801 / 49000) loss: 1.251040\n",
      "(Iteration 37901 / 49000) loss: 1.195811\n",
      "(Iteration 38001 / 49000) loss: 1.381852\n",
      "(Iteration 38101 / 49000) loss: 1.079804\n",
      "(Iteration 38201 / 49000) loss: 1.192188\n",
      "(Epoch 78 / 100) train acc: 0.704000; val_acc: 0.570000\n",
      "(Iteration 38301 / 49000) loss: 1.412786\n",
      "(Iteration 38401 / 49000) loss: 1.224444\n",
      "(Iteration 38501 / 49000) loss: 1.414266\n",
      "(Iteration 38601 / 49000) loss: 1.182147\n",
      "(Iteration 38701 / 49000) loss: 1.476404\n",
      "(Epoch 79 / 100) train acc: 0.703000; val_acc: 0.569000\n",
      "(Iteration 38801 / 49000) loss: 1.187167\n",
      "(Iteration 38901 / 49000) loss: 1.225803\n",
      "(Iteration 39001 / 49000) loss: 1.222245\n",
      "(Iteration 39101 / 49000) loss: 1.263856\n",
      "(Epoch 80 / 100) train acc: 0.699000; val_acc: 0.566000\n",
      "(Iteration 39201 / 49000) loss: 1.376314\n",
      "(Iteration 39301 / 49000) loss: 1.305123\n",
      "(Iteration 39401 / 49000) loss: 1.118729\n",
      "(Iteration 39501 / 49000) loss: 1.348352\n",
      "(Iteration 39601 / 49000) loss: 1.223449\n",
      "(Epoch 81 / 100) train acc: 0.712000; val_acc: 0.561000\n",
      "(Iteration 39701 / 49000) loss: 1.164901\n",
      "(Iteration 39801 / 49000) loss: 1.107750\n",
      "(Iteration 39901 / 49000) loss: 1.264644\n",
      "(Iteration 40001 / 49000) loss: 1.323900\n",
      "(Iteration 40101 / 49000) loss: 1.294580\n",
      "(Epoch 82 / 100) train acc: 0.711000; val_acc: 0.570000\n",
      "(Iteration 40201 / 49000) loss: 1.241538\n",
      "(Iteration 40301 / 49000) loss: 1.317553\n",
      "(Iteration 40401 / 49000) loss: 1.184874\n",
      "(Iteration 40501 / 49000) loss: 1.225441\n",
      "(Iteration 40601 / 49000) loss: 1.420180\n",
      "(Epoch 83 / 100) train acc: 0.730000; val_acc: 0.560000\n",
      "(Iteration 40701 / 49000) loss: 1.244994\n",
      "(Iteration 40801 / 49000) loss: 1.242609\n",
      "(Iteration 40901 / 49000) loss: 1.257652\n",
      "(Iteration 41001 / 49000) loss: 1.143715\n",
      "(Iteration 41101 / 49000) loss: 1.330193\n",
      "(Epoch 84 / 100) train acc: 0.702000; val_acc: 0.557000\n",
      "(Iteration 41201 / 49000) loss: 1.310609\n",
      "(Iteration 41301 / 49000) loss: 1.130862\n",
      "(Iteration 41401 / 49000) loss: 1.179864\n",
      "(Iteration 41501 / 49000) loss: 1.263600\n",
      "(Iteration 41601 / 49000) loss: 1.302591\n",
      "(Epoch 85 / 100) train acc: 0.722000; val_acc: 0.561000\n",
      "(Iteration 41701 / 49000) loss: 1.114122\n",
      "(Iteration 41801 / 49000) loss: 1.344065\n",
      "(Iteration 41901 / 49000) loss: 1.094616\n",
      "(Iteration 42001 / 49000) loss: 1.174317\n",
      "(Iteration 42101 / 49000) loss: 1.289154\n",
      "(Epoch 86 / 100) train acc: 0.714000; val_acc: 0.565000\n",
      "(Iteration 42201 / 49000) loss: 1.304225\n",
      "(Iteration 42301 / 49000) loss: 1.329462\n",
      "(Iteration 42401 / 49000) loss: 1.236677\n",
      "(Iteration 42501 / 49000) loss: 1.364641\n",
      "(Iteration 42601 / 49000) loss: 1.292095\n",
      "(Epoch 87 / 100) train acc: 0.716000; val_acc: 0.562000\n",
      "(Iteration 42701 / 49000) loss: 1.276949\n",
      "(Iteration 42801 / 49000) loss: 1.268955\n",
      "(Iteration 42901 / 49000) loss: 1.246157\n",
      "(Iteration 43001 / 49000) loss: 1.207971\n",
      "(Iteration 43101 / 49000) loss: 1.398394\n",
      "(Epoch 88 / 100) train acc: 0.719000; val_acc: 0.570000\n",
      "(Iteration 43201 / 49000) loss: 1.276965\n",
      "(Iteration 43301 / 49000) loss: 1.225396\n",
      "(Iteration 43401 / 49000) loss: 1.333135\n",
      "(Iteration 43501 / 49000) loss: 1.219091\n",
      "(Iteration 43601 / 49000) loss: 1.504462\n",
      "(Epoch 89 / 100) train acc: 0.712000; val_acc: 0.572000\n",
      "(Iteration 43701 / 49000) loss: 1.400364\n",
      "(Iteration 43801 / 49000) loss: 1.018677\n",
      "(Iteration 43901 / 49000) loss: 1.061666\n",
      "(Iteration 44001 / 49000) loss: 1.220184\n",
      "(Epoch 90 / 100) train acc: 0.695000; val_acc: 0.574000\n",
      "(Iteration 44101 / 49000) loss: 1.356095\n",
      "(Iteration 44201 / 49000) loss: 1.260883\n",
      "(Iteration 44301 / 49000) loss: 1.351917\n",
      "(Iteration 44401 / 49000) loss: 1.287263\n",
      "(Iteration 44501 / 49000) loss: 1.378059\n",
      "(Epoch 91 / 100) train acc: 0.710000; val_acc: 0.572000\n",
      "(Iteration 44601 / 49000) loss: 1.568144\n",
      "(Iteration 44701 / 49000) loss: 1.234102\n",
      "(Iteration 44801 / 49000) loss: 1.546108\n",
      "(Iteration 44901 / 49000) loss: 1.250091\n",
      "(Iteration 45001 / 49000) loss: 1.287754\n",
      "(Epoch 92 / 100) train acc: 0.721000; val_acc: 0.566000\n",
      "(Iteration 45101 / 49000) loss: 1.351667\n",
      "(Iteration 45201 / 49000) loss: 1.081193\n",
      "(Iteration 45301 / 49000) loss: 1.412171\n",
      "(Iteration 45401 / 49000) loss: 1.137280\n",
      "(Iteration 45501 / 49000) loss: 1.410340\n",
      "(Epoch 93 / 100) train acc: 0.705000; val_acc: 0.573000\n",
      "(Iteration 45601 / 49000) loss: 1.295622\n",
      "(Iteration 45701 / 49000) loss: 1.238185\n",
      "(Iteration 45801 / 49000) loss: 1.130132\n",
      "(Iteration 45901 / 49000) loss: 1.191031\n",
      "(Iteration 46001 / 49000) loss: 1.354855\n",
      "(Epoch 94 / 100) train acc: 0.733000; val_acc: 0.566000\n",
      "(Iteration 46101 / 49000) loss: 1.129235\n",
      "(Iteration 46201 / 49000) loss: 1.214314\n",
      "(Iteration 46301 / 49000) loss: 1.160897\n",
      "(Iteration 46401 / 49000) loss: 1.325706\n",
      "(Iteration 46501 / 49000) loss: 1.332131\n",
      "(Epoch 95 / 100) train acc: 0.705000; val_acc: 0.563000\n",
      "(Iteration 46601 / 49000) loss: 1.297670\n",
      "(Iteration 46701 / 49000) loss: 1.350244\n",
      "(Iteration 46801 / 49000) loss: 1.278606\n",
      "(Iteration 46901 / 49000) loss: 1.381003\n",
      "(Iteration 47001 / 49000) loss: 1.299355\n",
      "(Epoch 96 / 100) train acc: 0.729000; val_acc: 0.574000\n",
      "(Iteration 47101 / 49000) loss: 1.085617\n",
      "(Iteration 47201 / 49000) loss: 1.316970\n",
      "(Iteration 47301 / 49000) loss: 1.139961\n",
      "(Iteration 47401 / 49000) loss: 1.306852\n",
      "(Iteration 47501 / 49000) loss: 1.248851\n",
      "(Epoch 97 / 100) train acc: 0.718000; val_acc: 0.565000\n",
      "(Iteration 47601 / 49000) loss: 1.428204\n",
      "(Iteration 47701 / 49000) loss: 1.436008\n",
      "(Iteration 47801 / 49000) loss: 1.097861\n",
      "(Iteration 47901 / 49000) loss: 1.232278\n",
      "(Iteration 48001 / 49000) loss: 1.124019\n",
      "(Epoch 98 / 100) train acc: 0.706000; val_acc: 0.564000\n",
      "(Iteration 48101 / 49000) loss: 1.293397\n",
      "(Iteration 48201 / 49000) loss: 1.310619\n",
      "(Iteration 48301 / 49000) loss: 1.175811\n",
      "(Iteration 48401 / 49000) loss: 1.186158\n",
      "(Iteration 48501 / 49000) loss: 1.421746\n",
      "(Epoch 99 / 100) train acc: 0.708000; val_acc: 0.563000\n",
      "(Iteration 48601 / 49000) loss: 1.104001\n",
      "(Iteration 48701 / 49000) loss: 1.551079\n",
      "(Iteration 48801 / 49000) loss: 1.447654\n",
      "(Iteration 48901 / 49000) loss: 1.175318\n",
      "(Epoch 100 / 100) train acc: 0.705000; val_acc: 0.564000\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "lr = 3e-4#Learning rate\n",
    "reg = 1e-2#Regularization parameter\n",
    "hidden_dims = [200, 400, 400, 200]\n",
    "weight_scale = 5e-2\n",
    "num_epoch = 100\n",
    "batch_size = 100\n",
    "\n",
    "neural_net = FullyConnectedNet(hidden_dims, reg=reg, weight_scale=weight_scale, dropout=0.5, normalization='batchnorm')\n",
    "solver = Solver(neural_net, data,\n",
    "                  num_epochs=num_epoch, print_every=100, batch_size=batch_size,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': lr,\n",
    "                  }, lr_decay = 0.95,\n",
    "                  verbose=True)\n",
    "solver.train()\n",
    "\n",
    "best_model = neural_net\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.57\n",
      "Test set accuracy:  0.561\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
